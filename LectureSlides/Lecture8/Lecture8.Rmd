---
title: 'MPP-E1180 Lecture 8: Statistical Modeling with R'
author: "Christopher Gandrud"
date: "6 November 2014"
output:
  ioslides_presentation:
    css: http://maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css
    logo: https://raw.githubusercontent.com/christophergandrud/Hertie_Collab_Data_Science/master/img/HertieCollaborativeDataLogo_v1.png
  beamer_presentation: default
---

## <i class="fa fa-arrow-circle-o-up"></i> Objectives for the week

- Assignment 3

- Review

- Introduction to the general syntax for statistical modelling in R.

- Specific examples using:

    + Normal Linear regression

    + Logistic regression

## Assignment 3

**Purpose**: Gather, clean, and analyse data

**Deadline**: 14 November 2015

You will submit a GitHub repo that:

- Gathers web-based data from at least **two sources**. Cleans and merges the
data so that it is ready for statistical analyses.

- Conducts basic descriptive and inferential statistics with the data to address
a relevant research question.

- Briefly describes the results including with dynamically generated tables and
figures.

- Has a write up of 1,500 words maximum that describes the data gathering and analysis
and uses literate programming.

## Assignment 3

<br>
<br>
<br>

This is ideally a **good first run** at the data gathering and analysis for your
final project.

## Review

- What is web scraping? What are some of tools R has for web scraping?

- What are regular expressions (give at least two examples)? Why are they useful?

- What dplyr functions can you use to create a new variable in a data frame by
running a command on values from groups in that data frame?

## Statistical Modelling in R

**Caveat**: We are definitely not going to cover anywhere near R's full
capabilities for statistical modeling.

We are also not going to cover all of the modeling concerns/diagnostics you need
to consider when using a given model.

<i class="fa fa-question"></i> What are we going to do?

- Discuss the basic syntax and capabilities in R for estimating normal linear
and logistic regressions.

- Basic model checking.

- Discuss basic ways of interpreting results.

## The basic model

Most statistical models you will likely estimate are from a general class that
has **two parts**:

**Stocastic Component** that generates the dependent variable $Y_{i}$ as a
random draw from the probability density function:

$$
Y_{i} \sim f(\theta_{i},\: \alpha)
$$

- $\theta_{i}$: parameter vector of the part of the function that varies between observations.

- $\alpha$: matrix of non-varying parameters.

Sometimes referred to as the 'error structure'.

## The basic model

The **systematic component** indicating how $\theta_{i}$ varies across
observations depending on values of the explanatory variables and (often)
some constant:

$$
\theta_{i} = g(X_{i},\: \beta)
$$

- $X_{i}$: a $1\: \mathrm{x}\: k$ vector of explanatory variables.

- $\beta$: a $1\: \mathrm{x}\: k$ vector of parameters
(i.e. coefficients).

- $g(.,.)$: the "link function", specifying how the explanatory
variables and parameters are translated into $\theta_{i}$.

## Today

Today we will cover two variations of this general model:

- linear-normal regression (i.e. ordinary least squares)

- logit model

## Linear-normal regression

For continuous dependent variables assume that $Y_{i}$ is from the
 **normal distribution** ($N(.,.)$).

Set the main parameter vector $\theta_{i}$ to the scalar mean of: $\theta_{i} = E(y_{i})= \mu_{i}$.

Assume the ancillary parameter matrix is the scalar homoskedastic variance: $\alpha = V(Y_{i}) = \sigma^2$.

- **Homoskedastic variance**: variance does not depend on the value of $x$. The standard deviation of the error terms is constant across values of $x$.

Set the systematic component to the linear form:
$g(X_{i},\: \beta) = X_{i}\beta =  \beta_{0} + X_{i1}\beta_{1} + \ldots$.

## Linear-normal regression

<br>

So:

$$
Y_{i} \sim N(\mu_{i},\: \sigma^2), \:\:\: \mu_{i} = X_{i}\beta
$$

## Logit regression

For binary data (e.g. 0, 1) we can assume that the stochastic component has a
Bernoulli distribution.

The main parameter is $\pi_{i} = \mathrm{Pr}(Y_{i} = 1)$.

The systematic component is set to a logistic form:
$\pi_{i} = \frac{1}{1 + e^{-X_{i}\beta}}$.

So:

$$
Y_{i} \sim \mathrm{Bernoulli}(\pi_{i}), \:\:\: \pi_{i} = \frac{1}{1 + e^{-X_{i}\beta}}
$$

## R syntax

The general syntax for estimating statistical models in are is:


```{r, eval=FALSE}
response variable ~ explanatory variable(s)
```

Where `~` reads 'is modelled as a function of'.

## Model functions

We use model functions to specify the specific model structure.

Basic model functions include:

- `lm`: fits a linear model where $Y$ is assumed to be normally distributed 
and with homoskedastic variance.

- `glm`: allows the fitting of many Generalised Linear Models. Lets you specify 
the $Y$'s distribution `family` and the `link` function.

## Example of `lm` 

Example data *Prestige* (example based on 
<http://www.princeton.edu/~otorres/Regression101R.pdf>). 

The observations are **occupations** and the dependent variable is a score of 
each occupations **prestige**.

## Example of `lm`

```{r}
library(car)
data(Prestige)
summary(Prestige)
```

