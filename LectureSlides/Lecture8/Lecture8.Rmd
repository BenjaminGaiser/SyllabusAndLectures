---
title: 'MPP-E1180 Lecture 8: Statistical Modeling with R'
author: "Christopher Gandrud"
date: "6 November 2014"
output:
  beamer_presentation: default
  ioslides_presentation:
    css: http://maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css
    logo: https://raw.githubusercontent.com/christophergandrud/Hertie_Collab_Data_Science/master/img/HertieCollaborativeDataLogo_v1.png
---

## <i class="fa fa-arrow-circle-o-up"></i> Objectives for the week

- Assignment 3

- Review

- Intro to the general syntax for statistical modelling in R.

- Specific examples using:

    + Normal linear regression

    + Logistic regression
    
## Week 9

<br>
<br>
<br>

Class for Week 9 is on **10 November (Monday) from 14:00-16:00**.

## Assignment 3

**Purpose**: Gather, clean, and analyse data

**Deadline**: 14 November 2014

You will submit a GitHub repo that:

- Gathers web-based data from at least **two sources**. Cleans and merges the
data so that it is ready for statistical analyses.

- Conducts basic descriptive and inferential statistics with the data to address
a relevant research question.

- Briefly describes the results including with dynamically generated tables and
figures.

- Has a write up of 1,500 words maximum that describes the data gathering and
analysis and uses literate programming.

## Assignment 3

<br>
<br>
<br>

<i class="fa fa-exclamation"></i><i class="fa fa-exclamation"></i><i class="fa fa-exclamation"></i>
**Note**: I will be traveling/at a conference/not able to check my email much
on the **13th** and **14th**. 

So try to ask all of your **questions by the 12th (Wednesday)**.

I will have **normal office hours** on Wednesday.

## Review

- What is **web scraping**? What are some of tools R has for web scraping?

- What are **regular expressions** (give at least two examples)? Why are they
useful?

- What dplyr function can you use to create a **new variable** in a data frame by
running a command on values from groups in that data frame?

## Statistical Modelling in R

**Caveat**: We are **definitely not** going to cover anywhere near R's full
capabilities for statistical modeling.

We are also **not going to cover** all of the **modeling concerns/diagnostics** 
you need to consider when using a given model.

You will need to **rely on your other stats courses** and **texts**.

## <i class="fa fa-question"></i> What are we going to do?

- Discuss the basic syntax and capabilities in R for estimating normal linear
and logistic regressions.

- Basic model checking in R.

- Discuss basic ways of interpreting results (we'll do more on this next week).

## The basic model

Most statistical models you will estimate are from a general class that
has **two parts**:

**Stocastic Component** that assumes the dependent variable $Y_{i}$ is generated 
from as a random draw from the probability density function:

$$
Y_{i} \sim f(\theta_{i},\: \alpha)
$$

- $\theta_{i}$: parameter vector of the part of the function that **varies between
observations**.

- $\alpha$: matrix of **non-varying parameters**.

Sometimes referred to as the '**error structure**'.

## The basic model

The **Systematic Component** indicating how $\theta_{i}$ varies across
observations depending on values of the explanatory variables and (often)
some constant:

$$
\theta_{i} = g(X_{i},\: \beta)
$$

- $X_{i}$: a $1\: \mathrm{x}\: k$ vector of **explanatory variables**.

- $\beta$: a $1\: \mathrm{x}\: k$ vector of **parameters**
(i.e. coefficients).

- $g(.,.)$: the **link function**, specifying how the explanatory
variables and parameters are translated into $\theta_{i}$.

## Today

Today we will cover two variations of this general model:

- linear-normal regression (i.e. ordinary least squares)

- logit model

## Linear-normal regression

For continuous dependent variables assume that $Y_{i}$ is from the
 **normal distribution** ($N(.,.)$).

Set the main parameter vector $\theta_{i}$ to the **scalar mean** of:
$\theta_{i} = E(y_{i})= \mu_{i}$.

Assume the ancillary parameter matrix is the scalar homoskedastic variance:
$\alpha = V(Y_{i}) = \sigma^2$.

- **Homoskedastic variance**: variance does not depend on the value of $x$.
The standard deviation of the error terms is constant across values of $x$.

Set the systematic component to the linear form:
$g(X_{i},\: \beta) = X_{i}\beta =  \beta_{0} + X_{i1}\beta_{1} + \ldots$.

## Linear-normal regression

<br>

So:

$$
Y_{i} \sim N(\mu_{i},\: \sigma^2), \:\:\: \mu_{i} = X_{i}\beta
$$

## Logit regression

For binary data (e.g. 0, 1) we can assume that the stochastic component has a
Bernoulli distribution.

The main parameter is $\pi_{i} = \mathrm{Pr}(Y_{i} = 1)$.

The systematic component is set to a logistic form:
$\pi_{i} = \frac{1}{1 + e^{-X_{i}\beta}}$.

So:

$$
Y_{i} \sim \mathrm{Bernoulli}(\pi_{i}), \:\:\: \pi_{i} = \frac{1}{1 + e^{-X_{i}\beta}}
$$

## Error structure family and link function

| Error Family | Canonical link |
| ------------ | -------------- |
| Normal       | identity       |
| binomial     | logit          |
| poisson      | log            |

## R syntax

The general syntax for estimating statistical models in R is:


```{r, eval=FALSE}
response variable ~ explanatory variable(s)
```

Where '`~`' reads 'is modelled as a function of'.

## Model functions

We use model functions to specify the model structure.

Basic model functions include:

- `lm`: fits a linear model where $Y$ is assumed to be normally distributed
and with homoskedastic variance.

- `glm`: allows the fitting of many Generalised Linear Models. Lets you specify
the `family` and the `link` function.

## Example of `lm`

Example data *Prestige* (example based on
<http://www.princeton.edu/~otorres/Regression101R.pdf>).

The observations are **occupations** and the dependent variable is a score of
each occupations' **prestige**.

```{r}
library(car)
data(Prestige)
```

## Examine correlation matrix

```{r}
car::scatterplotMatrix(Prestige)
```


## Example of `lm`

Estimate simple model:

```{r}
M1 <- lm(prestige ~ education, data = Prestige)
```

---

```{r}
summary(M1)
```

## Confidence intervals of parameter point estimates

Note: **Always prefer estimation intervals** over point estimates.

Deal with your **uncertainty**!

About **95%** of the time the population parameter will be within **about 2 
standard errors** of the point estimate. 

Using **Central Limit Theorem** (at least about 50 observations and the data is not
extremely skewed):

$$
CI\_95 = \mathrm{point\: estimate} \pm 1.96 * SE
$$

## Confidence intervals of parameter point estimates

```{r}
confint(M1)
```

## Example of `lm`

Estimate model with categorical (factor) variable:

```{r}
M2 <- lm(prestige ~ education + type, 
         data = Prestige)
```

---

```{r}
summary(M2)
```

## Create categorical variable from continuous variable

Use the `cut` function to create a categorical (factor) variable from a
continuous variable.

```{r}
Prestige$income_cat <- cut(Prestige$income,
                breaks = c(0, 4999, 9999, 14999, 30000),
                labels = c('< 5,000', '< 10,000', '< 15,000',
                            '>= 15,000'))
summary(Prestige$income_cat)
```

Note: `cut` excludes the left value and includes the right value, e.g.
$(0,\: 4999]$.

## Example of `lm`

```{r}
M3 <- lm(prestige ~ education + income_cat, 
         data = Prestige)
confint(M3)
```

## Example of `lm`

Estimate models with polynomial transformations:

```{r}
# Cubic polynomial transformation
M4  <- lm(prestige ~ education + poly(income, 2), 
          data = Prestige)
confint(M4)
```

## Example of `lm`

Estimate models with (natural) logarithmic transformations:

```{r}
# Cubic polynomial transformation
M5  <- lm(prestige ~ education + log(income), 
          data = Prestige)
```

---
```{r}
summary(M5)
```

## Example of `lm`

Estimate model with interactions:

```{r}
M6 <- lm(prestige ~ education * type, 
         data = Prestige)
```

---

```{r}
summary(M6)
```

## Diagnose heteroscedasticity

Use `plot` on a model object to run visual diagnostics.

```{r}
plot(M2, which = 1)
```

## Diagnose non-normality of errors

`plot` to see if a model's errors are normally distributed.

```{r}
plot(M2, which = 2)
```

## Example of logistic regression with `glm`

Example from [UCLA IDRE](http://www.ats.ucla.edu/stat/r/dae/logit.htm).

Simulated data of admission to grad school.

```{r}
# Load data
URL <- 'http://www.ats.ucla.edu/stat/data/binary.csv'
Admission <- read.csv(URL)
```

## Example of logistic regression with `glm`

```{r, warning=FALSE, message=FALSE}
car::scatterplotMatrix(Admission)
```

## Contingency table for school rank and admission

```{r}
xtabs(~admit + rank, data = Admission)
```

```{r}
summary(xtabs(~admit + rank, data = Admission))
```

## Example of logistic regression with `glm`

```{r}
Logit1 <- glm(admit ~ gre + gpa + as.factor(rank),
              data = Admission, family = 'binomial')
```

Note: Link function is assumed to be logit if `family = 'binomial'`.

## Example of logistic regression with `glm`

```{r, message=FALSE}
confint(Logit1)
```

## Interpreting logistic regression results

$\beta$'s in logistic regression are interpretable as **log odds**. These are
weird.

If we exponentiate log odds we get **odds ratios**.

```{r, message=FALSE}
exp(cbind(OddsRatio = coef(Logit1), confint(Logit1)))
```

These are **also weird**.

## Interpreting logistic regression results

What we really want are **predicted probabilities**

**First** create a data frame of fitted values:

```{r}
fitted <- with(Admission, 
               data.frame(gre = mean(gre), 
                          gpa = mean(gpa),
                          rank = factor(1:4)))
fitted
```

## Interpreting logistic regression results

**Second** predict probability point estimates for each fitted value.

```{r}
fitted$predicted <- predict(Logit1, newdata = fitted,
                            type = 'response')

fitted
```


## More interpretation

Next week we will explore other methods of interpreting results from regression
models.

```{r, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}
Admission$rank <- as.factor(Admission$rank)

library(Zelig)
Z1 <- zelig(admit ~ gre + gpa + rank, cite = FALSE,
              data = Admission, model = 'logit')

setZ1 <- setx(Z1, gre = seq(220, 800, by = 1))

simZ1 <- sim(Z1, x = setZ1)

plot(simZ1)
```

## <i class="fa fa-arrow-circle-o-up"></i> Seminar: modeling

<br>
<br>

Begin working on the statistical models for **your project**.

and/or

**Out of Lecture Challenge**: Estimate a normal regression model and **plot 
predicted values** with uncertainty across a range of fitted values.

Functions you might find useful:

- `coef` and `confint`

